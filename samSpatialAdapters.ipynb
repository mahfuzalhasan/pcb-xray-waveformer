{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24ea3f50-54b6-4121-8be0-eb91b6f0b287",
   "metadata": {},
   "source": [
    "### final results by averaging the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9972a-1757-4aa0-8e25-e933d1c633c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Fold 1\n",
      "Fold 1, Epoch 1, Train Loss: 0.8538809621334076\n",
      "Fold 1, Epoch 1, Val Loss: 0.8579439039413745\n",
      "Fold 1, Epoch 2, Train Loss: 0.8457563352584839\n",
      "Fold 1, Epoch 2, Val Loss: 0.857499001117853\n",
      "Fold 1, Epoch 3, Train Loss: 0.8441890490055084\n",
      "Fold 1, Epoch 3, Val Loss: 0.8699133143975184\n",
      "Fold 1, Epoch 4, Train Loss: 0.8427632880210877\n",
      "Fold 1, Epoch 4, Val Loss: 0.853545755147934\n",
      "Fold 1, Epoch 5, Train Loss: 0.8410488909482956\n",
      "Fold 1, Epoch 5, Val Loss: 0.865165481200585\n",
      "Fold 1, Epoch 6, Train Loss: 0.8342749208211899\n",
      "Fold 1, Epoch 6, Val Loss: 0.8433061150404123\n",
      "Fold 1, Epoch 7, Train Loss: 0.8317443209886551\n",
      "Fold 1, Epoch 7, Val Loss: 0.8352242158009455\n",
      "Fold 1, Epoch 8, Train Loss: 0.830240592956543\n",
      "Fold 1, Epoch 8, Val Loss: 0.8415387410383958\n",
      "Fold 1, Epoch 9, Train Loss: 0.8290007108449936\n",
      "Fold 1, Epoch 9, Val Loss: 0.8384403311289274\n",
      "Fold 1, Epoch 10, Train Loss: 0.8281853550672531\n",
      "Fold 1, Epoch 10, Val Loss: 0.8470728122271024\n",
      "Fold 1, Epoch 11, Train Loss: 0.8270949989557266\n",
      "Fold 1, Epoch 11, Val Loss: 0.8299107712048751\n",
      "Fold 1, Epoch 12, Train Loss: 0.8263079857826233\n",
      "Fold 1, Epoch 12, Val Loss: 0.8261595505934495\n",
      "Fold 1, Epoch 13, Train Loss: 0.8251882058382034\n",
      "Fold 1, Epoch 13, Val Loss: 0.8261133271914262\n",
      "Fold 1, Epoch 14, Train Loss: 0.8243335765600205\n",
      "Fold 1, Epoch 14, Val Loss: 0.8261873630376962\n",
      "Fold 1, Epoch 15, Train Loss: 0.8242182469367981\n",
      "Fold 1, Epoch 15, Val Loss: 0.8311749054835393\n",
      "Fold 1, Epoch 16, Train Loss: 0.8232651835680008\n",
      "Fold 1, Epoch 16, Val Loss: 0.875976908665437\n",
      "Fold 1, Epoch 17, Train Loss: 0.8222743457555771\n",
      "Fold 1, Epoch 17, Val Loss: 0.828691881436568\n",
      "Fold 1, Epoch 18, Train Loss: 0.8218313068151474\n",
      "Fold 1, Epoch 18, Val Loss: 0.8396725746301504\n",
      "Fold 1, Epoch 19, Train Loss: 0.8215761035680771\n",
      "Fold 1, Epoch 19, Val Loss: 0.8215018166945531\n",
      "Fold 1, Epoch 20, Train Loss: 0.8202896779775619\n",
      "Fold 1, Epoch 20, Val Loss: 0.819562737758343\n",
      "Fold 1, Epoch 21, Train Loss: 0.8195657402276992\n",
      "Fold 1, Epoch 21, Val Loss: 0.8190878858933082\n",
      "Fold 1, Epoch 22, Train Loss: 0.8189694476127625\n",
      "Fold 1, Epoch 22, Val Loss: 0.819026903464244\n",
      "Fold 1, Epoch 23, Train Loss: 0.8183327698707581\n",
      "Fold 1, Epoch 23, Val Loss: 0.8210557240706223\n",
      "Fold 1, Epoch 24, Train Loss: 0.817745811343193\n",
      "Fold 1, Epoch 24, Val Loss: 0.8206142783164978\n",
      "Fold 1, Epoch 25, Train Loss: 0.8172100150585174\n",
      "Fold 1, Epoch 25, Val Loss: 0.828739698116596\n",
      "Fold 2\n",
      "Fold 2, Epoch 1, Train Loss: 0.8523349160014992\n",
      "Fold 2, Epoch 1, Val Loss: 0.8367728161811828\n",
      "Fold 2, Epoch 2, Train Loss: 0.8418021202087402\n",
      "Fold 2, Epoch 2, Val Loss: 0.8452924275398255\n",
      "Fold 2, Epoch 3, Train Loss: 0.8403841658393936\n",
      "Fold 2, Epoch 3, Val Loss: 0.8415297842025757\n",
      "Fold 2, Epoch 4, Train Loss: 0.838969618967264\n",
      "Fold 2, Epoch 4, Val Loss: 0.8400771832466125\n",
      "Fold 2, Epoch 5, Train Loss: 0.8378261593308779\n",
      "Fold 2, Epoch 5, Val Loss: 0.8390053915977478\n",
      "Fold 2, Epoch 6, Train Loss: 0.8366560056658074\n",
      "Fold 2, Epoch 6, Val Loss: 0.8368938970565796\n",
      "Fold 2, Epoch 7, Train Loss: 0.8356449137819876\n",
      "Fold 2, Epoch 7, Val Loss: 0.8450192737579346\n",
      "Fold 2, Epoch 8, Train Loss: 0.8347353917537349\n",
      "Fold 2, Epoch 8, Val Loss: 0.8357588791847229\n",
      "Fold 2, Epoch 9, Train Loss: 0.8337165241194243\n",
      "Fold 2, Epoch 9, Val Loss: 0.8343427133560181\n",
      "Fold 2, Epoch 10, Train Loss: 0.8328628239065113\n",
      "Fold 2, Epoch 10, Val Loss: 0.832597553730011\n",
      "Fold 2, Epoch 11, Train Loss: 0.832182004900262\n",
      "Fold 2, Epoch 11, Val Loss: 0.8315126585960388\n",
      "Fold 2, Epoch 12, Train Loss: 0.831102476851775\n",
      "Fold 2, Epoch 12, Val Loss: 0.8306125473976135\n",
      "Fold 2, Epoch 13, Train Loss: 0.8302273644079076\n",
      "Fold 2, Epoch 13, Val Loss: 0.8308674931526184\n",
      "Fold 2, Epoch 14, Train Loss: 0.8294422231098213\n",
      "Fold 2, Epoch 14, Val Loss: 0.8486808252334594\n",
      "Fold 2, Epoch 15, Train Loss: 0.8286161824028091\n",
      "Fold 2, Epoch 15, Val Loss: 0.8279719829559327\n",
      "Fold 2, Epoch 16, Train Loss: 0.8278129838480808\n",
      "Fold 2, Epoch 16, Val Loss: 0.8283627319335938\n",
      "Fold 2, Epoch 17, Train Loss: 0.8270490458696196\n",
      "Fold 2, Epoch 17, Val Loss: 0.8264168858528137\n",
      "Fold 2, Epoch 18, Train Loss: 0.8264541614173663\n",
      "Fold 2, Epoch 18, Val Loss: 0.8259396123886108\n",
      "Fold 2, Epoch 19, Train Loss: 0.8255135629436757\n",
      "Fold 2, Epoch 19, Val Loss: 0.8261246538162231\n",
      "Fold 2, Epoch 20, Train Loss: 0.8248225190851948\n",
      "Fold 2, Epoch 20, Val Loss: 0.8248212671279908\n",
      "Fold 2, Epoch 21, Train Loss: 0.8241150615238907\n",
      "Fold 2, Epoch 21, Val Loss: 0.8243180775642395\n",
      "Fold 2, Epoch 22, Train Loss: 0.8234676824937953\n",
      "Fold 2, Epoch 22, Val Loss: 0.8235842943191528\n",
      "Fold 2, Epoch 23, Train Loss: 0.822799274826994\n",
      "Fold 2, Epoch 23, Val Loss: 0.822820942401886\n",
      "Fold 2, Epoch 24, Train Loss: 0.8221465467226388\n",
      "Fold 2, Epoch 24, Val Loss: 0.8215490078926087\n",
      "Fold 2, Epoch 25, Train Loss: 0.8215149552515237\n",
      "Fold 2, Epoch 25, Val Loss: 0.8213487100601197\n",
      "Fold 3\n",
      "Fold 3, Epoch 1, Train Loss: 0.8570618552736716\n",
      "Fold 3, Epoch 1, Val Loss: 0.8715759348869324\n",
      "Fold 3, Epoch 2, Train Loss: 0.8453768963860994\n",
      "Fold 3, Epoch 2, Val Loss: 0.8768428778648376\n",
      "Fold 3, Epoch 3, Train Loss: 0.8433111664092187\n",
      "Fold 3, Epoch 3, Val Loss: 0.8587171053886413\n",
      "Fold 3, Epoch 4, Train Loss: 0.841881784472135\n",
      "Fold 3, Epoch 4, Val Loss: 0.8573230433464051\n",
      "Fold 3, Epoch 5, Train Loss: 0.8392207935304925\n",
      "Fold 3, Epoch 5, Val Loss: 0.8612885546684265\n",
      "Fold 3, Epoch 6, Train Loss: 0.8332552455439426\n",
      "Fold 3, Epoch 6, Val Loss: 0.8527374672889709\n",
      "Fold 3, Epoch 7, Train Loss: 0.8312651745163568\n",
      "Fold 3, Epoch 7, Val Loss: 0.8369240188598632\n",
      "Fold 3, Epoch 8, Train Loss: 0.8299796168166812\n",
      "Fold 3, Epoch 8, Val Loss: 0.835732364654541\n",
      "Fold 3, Epoch 9, Train Loss: 0.8290019383524904\n",
      "Fold 3, Epoch 9, Val Loss: 0.8505631494522095\n",
      "Fold 3, Epoch 10, Train Loss: 0.8283509252094986\n",
      "Fold 3, Epoch 10, Val Loss: 0.8672950744628907\n",
      "Fold 3, Epoch 11, Train Loss: 0.8273014066242935\n",
      "Fold 3, Epoch 11, Val Loss: 0.8451986813545227\n",
      "Fold 3, Epoch 12, Train Loss: 0.8261357982559959\n",
      "Fold 3, Epoch 12, Val Loss: 0.8285672569274902\n",
      "Fold 3, Epoch 13, Train Loss: 0.825348428570398\n",
      "Fold 3, Epoch 13, Val Loss: 0.8306403636932373\n",
      "Fold 3, Epoch 14, Train Loss: 0.8245920361858783\n",
      "Fold 3, Epoch 14, Val Loss: 0.8353212404251099\n",
      "Fold 3, Epoch 15, Train Loss: 0.8238301973531742\n",
      "Fold 3, Epoch 15, Val Loss: 0.8246823644638062\n",
      "Fold 3, Epoch 16, Train Loss: 0.8229233085519017\n",
      "Fold 3, Epoch 16, Val Loss: 0.8235120439529419\n",
      "Fold 3, Epoch 17, Train Loss: 0.8221769315181392\n",
      "Fold 3, Epoch 17, Val Loss: 0.8223799085617065\n",
      "Fold 3, Epoch 18, Train Loss: 0.8212876744789652\n",
      "Fold 3, Epoch 18, Val Loss: 0.8217297625541687\n",
      "Fold 3, Epoch 19, Train Loss: 0.8207546732213238\n",
      "Fold 3, Epoch 19, Val Loss: 0.8203499245643616\n",
      "Fold 3, Epoch 20, Train Loss: 0.8200457668540502\n",
      "Fold 3, Epoch 20, Val Loss: 0.8196532392501831\n",
      "Fold 3, Epoch 21, Train Loss: 0.8193382986701361\n",
      "Fold 3, Epoch 21, Val Loss: 0.8199105715751648\n",
      "Fold 3, Epoch 22, Train Loss: 0.8187099919460787\n",
      "Fold 3, Epoch 22, Val Loss: 0.8188059616088867\n",
      "Fold 3, Epoch 23, Train Loss: 0.818343717272919\n",
      "Fold 3, Epoch 23, Val Loss: 0.8234621906280517\n",
      "Fold 3, Epoch 24, Train Loss: 0.8177180443659867\n",
      "Fold 3, Epoch 24, Val Loss: 0.8167656946182251\n",
      "Fold 3, Epoch 25, Train Loss: 0.8170669633563202\n",
      "Fold 3, Epoch 25, Val Loss: 0.8163978719711303\n",
      "Fold 4\n",
      "Fold 4, Epoch 1, Train Loss: 0.8579932321416269\n",
      "Fold 4, Epoch 1, Val Loss: 0.8586496043205262\n",
      "Fold 4, Epoch 2, Train Loss: 0.8498715238996072\n",
      "Fold 4, Epoch 2, Val Loss: 0.8623775148391724\n",
      "Fold 4, Epoch 3, Train Loss: 0.8485345746030902\n",
      "Fold 4, Epoch 3, Val Loss: 0.8523448848724365\n",
      "Fold 4, Epoch 4, Train Loss: 0.8475013457902587\n",
      "Fold 4, Epoch 4, Val Loss: 0.8602404808998108\n",
      "Fold 4, Epoch 5, Train Loss: 0.8463854429745438\n",
      "Fold 4, Epoch 5, Val Loss: 0.8507801651954651\n",
      "Fold 4, Epoch 6, Train Loss: 0.8453312291957364\n",
      "Fold 4, Epoch 6, Val Loss: 0.8400358533859253\n",
      "Fold 4, Epoch 7, Train Loss: 0.8444439261266501\n",
      "Fold 4, Epoch 7, Val Loss: 0.8527694964408874\n",
      "Fold 4, Epoch 8, Train Loss: 0.8433393989459123\n",
      "Fold 4, Epoch 8, Val Loss: 0.8460938286781311\n",
      "Fold 4, Epoch 9, Train Loss: 0.8425751706161121\n",
      "Fold 4, Epoch 9, Val Loss: 0.8448163795471192\n",
      "Fold 4, Epoch 10, Train Loss: 0.8416457961101343\n",
      "Fold 4, Epoch 10, Val Loss: 0.860434217453003\n",
      "Fold 4, Epoch 11, Train Loss: 0.8406010659614412\n",
      "Fold 4, Epoch 11, Val Loss: 0.8349708080291748\n",
      "Fold 4, Epoch 12, Train Loss: 0.8394856954565143\n",
      "Fold 4, Epoch 12, Val Loss: 0.8356838512420655\n",
      "Fold 4, Epoch 13, Train Loss: 0.8384486419139522\n",
      "Fold 4, Epoch 13, Val Loss: 0.8538411784172059\n",
      "Fold 4, Epoch 14, Train Loss: 0.8370457179475539\n",
      "Fold 4, Epoch 14, Val Loss: 0.8930789303779602\n",
      "Fold 4, Epoch 15, Train Loss: 0.8358813754402765\n",
      "Fold 4, Epoch 15, Val Loss: 0.8396273469924926\n",
      "Fold 4, Epoch 16, Train Loss: 0.8339656072087808\n",
      "Fold 4, Epoch 16, Val Loss: 0.8359024786949157\n",
      "Fold 4, Epoch 17, Train Loss: 0.8309167948099646\n",
      "Fold 4, Epoch 17, Val Loss: 0.8493065738677978\n",
      "Fold 4, Epoch 18, Train Loss: 0.8289825568104735\n",
      "Fold 4, Epoch 18, Val Loss: 0.836121428012848\n",
      "Fold 4, Epoch 19, Train Loss: 0.8275998978331538\n",
      "Fold 4, Epoch 19, Val Loss: 0.8240867948532105\n",
      "Fold 4, Epoch 20, Train Loss: 0.826517931305536\n",
      "Fold 4, Epoch 20, Val Loss: 0.8315573620796204\n",
      "Fold 4, Epoch 21, Train Loss: 0.8253151514742634\n",
      "Fold 4, Epoch 21, Val Loss: 0.8238217639923096\n",
      "Fold 4, Epoch 22, Train Loss: 0.8243141699545454\n",
      "Fold 4, Epoch 22, Val Loss: 0.8213179111480713\n",
      "Fold 4, Epoch 23, Train Loss: 0.8233053861278119\n",
      "Fold 4, Epoch 23, Val Loss: 0.8298390889167786\n",
      "Fold 4, Epoch 24, Train Loss: 0.8224550084312363\n",
      "Fold 4, Epoch 24, Val Loss: 0.8297341346740723\n",
      "Fold 4, Epoch 25, Train Loss: 0.8216498948559903\n",
      "Fold 4, Epoch 25, Val Loss: 0.8190155100822448\n",
      "Fold 5\n",
      "Fold 5, Epoch 1, Train Loss: 0.8547438949641615\n",
      "Fold 5, Epoch 1, Val Loss: 0.8404296374320984\n",
      "Fold 5, Epoch 2, Train Loss: 0.845049279751164\n",
      "Fold 5, Epoch 2, Val Loss: 0.8409038257598876\n",
      "Fold 5, Epoch 3, Train Loss: 0.8438238413027017\n",
      "Fold 5, Epoch 3, Val Loss: 0.8449000525474548\n",
      "Fold 5, Epoch 4, Train Loss: 0.842851461160301\n",
      "Fold 5, Epoch 4, Val Loss: 0.8461628413200378\n",
      "Fold 5, Epoch 5, Train Loss: 0.8420086597452069\n",
      "Fold 5, Epoch 5, Val Loss: 0.8395437598228455\n",
      "Fold 5, Epoch 6, Train Loss: 0.8410143869938237\n",
      "Fold 5, Epoch 6, Val Loss: 0.8396546769142151\n",
      "Fold 5, Epoch 7, Train Loss: 0.84019537904475\n",
      "Fold 5, Epoch 7, Val Loss: 0.8454876661300659\n",
      "Fold 5, Epoch 8, Train Loss: 0.8392658593631027\n",
      "Fold 5, Epoch 8, Val Loss: 0.8366393566131591\n",
      "Fold 5, Epoch 9, Train Loss: 0.8383643332094249\n",
      "Fold 5, Epoch 9, Val Loss: 0.8371245193481446\n",
      "Fold 5, Epoch 10, Train Loss: 0.8375174887109511\n",
      "Fold 5, Epoch 10, Val Loss: 0.836634657382965\n",
      "Fold 5, Epoch 11, Train Loss: 0.8370574464892396\n",
      "Fold 5, Epoch 11, Val Loss: 0.8362285995483398\n",
      "Fold 5, Epoch 12, Train Loss: 0.836056130947453\n",
      "Fold 5, Epoch 12, Val Loss: 0.8350085186958313\n",
      "Fold 5, Epoch 13, Train Loss: 0.835182774184954\n",
      "Fold 5, Epoch 13, Val Loss: 0.8346278619766235\n",
      "Fold 5, Epoch 14, Train Loss: 0.8344263345888345\n",
      "Fold 5, Epoch 14, Val Loss: 0.833895161151886\n",
      "Fold 5, Epoch 15, Train Loss: 0.8336392434516756\n",
      "Fold 5, Epoch 15, Val Loss: 0.8332544994354248\n",
      "Fold 5, Epoch 16, Train Loss: 0.8329332954812758\n",
      "Fold 5, Epoch 16, Val Loss: 0.8325737261772156\n",
      "Fold 5, Epoch 17, Train Loss: 0.832327676881658\n",
      "Fold 5, Epoch 17, Val Loss: 0.8990733861923218\n",
      "Fold 5, Epoch 18, Train Loss: 0.8316455052630736\n",
      "Fold 5, Epoch 18, Val Loss: 0.8300660729408265\n",
      "Fold 5, Epoch 19, Train Loss: 0.8310083262991197\n",
      "Fold 5, Epoch 19, Val Loss: 0.8347064828872681\n",
      "Fold 5, Epoch 20, Train Loss: 0.8302355095891669\n",
      "Fold 5, Epoch 20, Val Loss: 0.8288833451271057\n",
      "Fold 5, Epoch 21, Train Loss: 0.8302223340119466\n",
      "Fold 5, Epoch 21, Val Loss: 0.8190875601768494\n",
      "Fold 5, Epoch 22, Train Loss: 0.8289639318343436\n",
      "Fold 5, Epoch 22, Val Loss: 0.826586103439331\n",
      "Fold 5, Epoch 23, Train Loss: 0.8280750943882631\n",
      "Fold 5, Epoch 23, Val Loss: 0.8242394089698791\n",
      "Fold 5, Epoch 24, Train Loss: 0.8272788424303036\n",
      "Fold 5, Epoch 24, Val Loss: 0.8240767216682434\n",
      "Fold 5, Epoch 25, Train Loss: 0.8264388666294589\n",
      "Fold 5, Epoch 25, Val Loss: 0.8238760685920715\n",
      "Cross-validation results: [{'fold': 1, 'train_loss': 0.8172100150585174, 'val_loss': 0.828739698116596, 'best_model_path': 'model_results/resultsEpoch25_withoutCP/fold_1/best_model_epoch_22.pth', 'mean_dice': 0.49293588047555387, 'mean_precision': 0.2, 'mean_recall': 0.19859522306002103}, {'fold': 2, 'train_loss': 0.8215149552515237, 'val_loss': 0.8213487100601197, 'best_model_path': 'model_results/resultsEpoch25_withoutCP/fold_2/best_model_epoch_25.pth', 'mean_dice': 0.661696131105489, 'mean_precision': 0.2, 'mean_recall': 0.19917501831054688}, {'fold': 3, 'train_loss': 0.8170669633563202, 'val_loss': 0.8163978719711303, 'best_model_path': 'model_results/resultsEpoch25_withoutCP/fold_3/best_model_epoch_25.pth', 'mean_dice': 0.7653615131068806, 'mean_precision': 0.2, 'mean_recall': 0.199120361328125}, {'fold': 4, 'train_loss': 0.8216498948559903, 'val_loss': 0.8190155100822448, 'best_model_path': 'model_results/resultsEpoch25_withoutCP/fold_4/best_model_epoch_25.pth', 'mean_dice': 1.0, 'mean_precision': 0.2, 'mean_recall': 0.2}, {'fold': 5, 'train_loss': 0.8264388666294589, 'val_loss': 0.8238760685920715, 'best_model_path': 'model_results/resultsEpoch25_withoutCP/fold_5/best_model_epoch_21.pth', 'mean_dice': 0.6363423622436343, 'mean_precision': 0.2, 'mean_recall': 0.17832086944580078}]\n",
      "Average Train Loss: 0.8207761390303621\n",
      "Average Val Loss: 0.8218755717644324\n",
      "Average Dice Score: 0.7112671773863115\n",
      "Average Precision: 0.2\n",
      "Average Recall: 0.1950422944288987\n",
      "Best model based on Dice Score: model_results/resultsEpoch25_withoutCP/fold_4/best_model_epoch_25.pth with score 1.0\n",
      "Best model based on Precision: model_results/resultsEpoch25_withoutCP/fold_1/best_model_epoch_22.pth with score 0.2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask\n",
    "from transformers import SamModel, SamProcessor, SamConfig\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Determine the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "image_folder_path = \"Images/\"\n",
    "annotation_path = \"annotation/pcb1to15.json\"\n",
    "output_base_path = \"model_results/resultsEpoch25_withoutCP/\"\n",
    "\n",
    "# Load the annotations\n",
    "with open(annotation_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Get all image file names\n",
    "image_files = [img['file_name'] for img in coco_data['images']]\n",
    "\n",
    "# Helper function to save split annotations\n",
    "def save_split_annotations(file_names, coco_data, output_path):\n",
    "    # Filter images\n",
    "    images = [img for img in coco_data['images'] if img['file_name'] in file_names]\n",
    "    image_ids = [img['id'] for img in images]\n",
    "\n",
    "    # Filter annotations\n",
    "    annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] in image_ids]\n",
    "\n",
    "    # Create new COCO data dictionary\n",
    "    new_coco_data = {\n",
    "        'info': coco_data['info'],\n",
    "        'licenses': coco_data['licenses'],\n",
    "        'images': images,\n",
    "        'annotations': annotations,\n",
    "        'categories': coco_data['categories']\n",
    "    }\n",
    "\n",
    "    # Save new annotations\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(new_coco_data, f, indent=4)\n",
    "\n",
    "# Helper function to copy images to their respective folders\n",
    "def copy_images(file_names, source_folder, destination_folder):\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    for file_name in file_names:\n",
    "        shutil.copy(os.path.join(source_folder, file_name), os.path.join(destination_folder, file_name))\n",
    "\n",
    "# Define data augmentation and normalization transforms\n",
    "class CustomTransforms:\n",
    "    def __init__(self, is_train=True):\n",
    "        if is_train:\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((1024, 1024)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((1024, 1024)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        image = self.transforms(image)\n",
    "        mask = Image.fromarray(mask)\n",
    "        mask = F.resize(mask, (1024, 1024), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        mask = transforms.ToTensor()(mask).long().squeeze(0)  # Ensure mask is 2D\n",
    "        return image, mask\n",
    "\n",
    "# Custom dataset class\n",
    "class PCBXRayDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        self.image_info = self.coco_data['images']\n",
    "        self.annotations = {ann['image_id']: ann for ann in self.coco_data['annotations']}\n",
    "        self.category_info = self.coco_data['categories']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.image_info[idx]\n",
    "        image_id = img_info['id']\n",
    "        image_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "        anns = [ann for ann in self.annotations.values() if ann['image_id'] == image_id]\n",
    "        for ann in anns:\n",
    "            rle = coco_mask.frPyObjects(ann['segmentation'], image.shape[0], image.shape[1])\n",
    "            decoded_mask = coco_mask.decode(rle)\n",
    "            if len(decoded_mask.shape) == 3:\n",
    "                decoded_mask = decoded_mask[:, :, 0]  # Take the first channel if it's a multi-channel mask\n",
    "            mask += np.squeeze(decoded_mask).astype(np.uint8)\n",
    "\n",
    "        if self.transforms:\n",
    "            image, mask = self.transforms(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "class SpatialAdapter(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SpatialAdapter, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "class SAMWithSpatialAdapters(nn.Module):\n",
    "    def __init__(self, sam_model, adapter_channels=256, num_classes=5):  # Add num_classes parameter\n",
    "        super(SAMWithSpatialAdapters, self).__init__()\n",
    "        self.sam_model = sam_model\n",
    "        # Initialize SpatialAdapter with 256 input and output channels\n",
    "        self.adapters = nn.ModuleList([SpatialAdapter(256, adapter_channels) for _ in range(sam_model.config.vision_config.num_hidden_layers)])\n",
    "        self.final_conv = nn.Conv2d(adapter_channels, num_classes, kernel_size=1)  # Change to num_classes\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        vision_outputs = self.sam_model.vision_encoder(pixel_values)\n",
    "        x = vision_outputs[0]  \n",
    "        # print(f\"vision_outputs[0] shape: {x.shape}\")  # Debug-- to inspect shape\n",
    "        for adapter in self.adapters:\n",
    "            x = adapter(x)\n",
    "        x = self.final_conv(x)  \n",
    "        # print(f\"Final output shape before upsampling: {x.shape}\")  # Debug - to inspect shape\n",
    "        return x\n",
    "\n",
    "# Load the pre-trained SAM model\n",
    "sam_model = SamModel.from_pretrained('facebook/sam-vit-huge')\n",
    "\n",
    "# Integrate spatial adapters\n",
    "model_with_adapters = SAMWithSpatialAdapters(sam_model).to(device)\n",
    "\n",
    "# Set up the optimizer with Layer-wise Learning Rate Decay\n",
    "def get_optimizer_with_llrd(model, base_lr=1e-4, lr_decay_factor=0.95):\n",
    "    seen_params = set()\n",
    "    optimizer_grouped_parameters = []\n",
    "    for i, layer in enumerate(model.sam_model.vision_encoder.layers):\n",
    "        layer_params = [p for n, p in layer.named_parameters() if p not in seen_params]\n",
    "        seen_params.update(layer_params)\n",
    "        optimizer_grouped_parameters.append({'params': layer_params, 'lr': base_lr * (lr_decay_factor ** (len(model.sam_model.vision_encoder.layers) - i))})\n",
    "\n",
    "    adapter_params = [p for n, p in model.named_parameters() if \"adapter\" in n and p not in seen_params]\n",
    "    seen_params.update(adapter_params)\n",
    "    optimizer_grouped_parameters.append({'params': adapter_params, 'lr': base_lr})\n",
    "\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "    return optimizer\n",
    "\n",
    "optimizer = get_optimizer_with_llrd(model_with_adapters)\n",
    "\n",
    "# Dice Loss Function for Multiple Classes\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = torch.softmax(inputs, dim=1)  # multi-class segmentation\n",
    "        loss = 0\n",
    "        for c in range(self.num_classes):\n",
    "            input_c = inputs[:, c, :, :]\n",
    "            target_c = (targets == c).float()\n",
    "            intersection = (input_c * target_c).sum()\n",
    "            dice = (2. * intersection + smooth) / (input_c.sum() + target_c.sum() + smooth)\n",
    "            loss += 1 - dice\n",
    "        return loss / self.num_classes\n",
    "\n",
    "# Training function\n",
    "def train_model(train_loader, val_loader, fold, num_epochs=25, num_classes=5):\n",
    "    model = SAMWithSpatialAdapters(sam_model, num_classes=num_classes).to(device)\n",
    "    criterion = DiceLoss(num_classes=num_classes)\n",
    "    optimizer = get_optimizer_with_llrd(model)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            outputs = torch.nn.functional.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Debug prints\n",
    "            #print(f\"Epoch {epoch + 1}, Fold {fold + 1}, outputs shape: {outputs.shape}, masks shape: {masks.shape}\")\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}, Train Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                outputs = model(images)\n",
    "                outputs = torch.nn.functional.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "                # Debug prints\n",
    "                #print(f\"Epoch {epoch + 1}, Fold {fold + 1}, outputs shape: {outputs.shape}, masks shape: {masks.shape}\")\n",
    "\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}, Val Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = os.path.join(fold_output_path, f'best_model_epoch_{epoch + 1}.pth')\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "    return train_loss / len(train_loader), val_loss / len(val_loader), best_model_path\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, num_classes=5):\n",
    "    model.eval()\n",
    "    dice_scores = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in data_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.nn.functional.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            outputs = torch.argmax(outputs, dim=1)  # Get the class with the highest score\n",
    "\n",
    "            for c in range(num_classes):\n",
    "                pred_mask = (outputs == c).float()\n",
    "                true_mask = (masks == c).float()\n",
    "                dice = dice_score(pred_mask.cpu().numpy(), true_mask.cpu().numpy())\n",
    "                dice_scores.append(dice)\n",
    "\n",
    "                precision = precision_score(true_mask.cpu().numpy().flatten(), pred_mask.cpu().numpy().flatten(), zero_division=0)\n",
    "                recall = recall_score(true_mask.cpu().numpy().flatten(), pred_mask.cpu().numpy().flatten(), zero_division=0)\n",
    "\n",
    "                precision_list.append(precision)\n",
    "                recall_list.append(recall)\n",
    "\n",
    "    mean_dice = np.mean(dice_scores)\n",
    "    mean_precision = np.mean(precision_list)\n",
    "    mean_recall = np.mean(recall_list)\n",
    "\n",
    "    return mean_dice, mean_precision, mean_recall\n",
    "\n",
    "# Dice score function\n",
    "def dice_score(pred, target, smooth=1):\n",
    "    intersection = np.sum(pred * target)\n",
    "    return (2. * intersection + smooth) / (np.sum(pred) + np.sum(target) + smooth)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "best_model_dice = {'score': 0, 'path': None}\n",
    "best_model_precision = {'score': 0, 'path': None}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(image_files)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    train_files = [image_files[i] for i in train_idx]\n",
    "    val_files = [image_files[i] for i in val_idx]\n",
    "\n",
    "    fold_output_path = os.path.join(output_base_path, f'fold_{fold + 1}')\n",
    "    train_image_path = os.path.join(fold_output_path, 'train')\n",
    "    val_image_path = os.path.join(fold_output_path, 'val')\n",
    "\n",
    "    os.makedirs(train_image_path, exist_ok=True)\n",
    "    os.makedirs(val_image_path, exist_ok=True)\n",
    "\n",
    "    save_split_annotations(train_files, coco_data, os.path.join(fold_output_path, 'train_annotations.json'))\n",
    "    save_split_annotations(val_files, coco_data, os.path.join(fold_output_path, 'val_annotations.json'))\n",
    "    copy_images(train_files, image_folder_path, train_image_path)\n",
    "    copy_images(val_files, image_folder_path, val_image_path)\n",
    "\n",
    "    train_dataset = PCBXRayDataset(train_image_path, os.path.join(fold_output_path, 'train_annotations.json'), transforms=CustomTransforms(is_train=True))\n",
    "    val_dataset = PCBXRayDataset(val_image_path, os.path.join(fold_output_path, 'val_annotations.json'), transforms=CustomTransforms(is_train=False))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    train_loss, val_loss, best_model_path = train_model(train_loader, val_loader, fold, num_classes=5)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model_with_adapters.load_state_dict(torch.load(best_model_path))\n",
    "    mean_dice, mean_precision, mean_recall = evaluate_model(model_with_adapters, val_loader, num_classes=5)\n",
    "\n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'best_model_path': best_model_path,\n",
    "        'mean_dice': mean_dice,\n",
    "        'mean_precision': mean_precision,\n",
    "        'mean_recall': mean_recall,\n",
    "    })\n",
    "\n",
    "    if mean_dice > best_model_dice['score']:\n",
    "        best_model_dice['score'] = mean_dice\n",
    "        best_model_dice['path'] = best_model_path\n",
    "\n",
    "    if mean_precision > best_model_precision['score']:\n",
    "        best_model_precision['score'] = mean_precision\n",
    "        best_model_precision['path'] = best_model_path\n",
    "\n",
    "# Calculate average results across all folds\n",
    "avg_train_loss = np.mean([result['train_loss'] for result in fold_results])\n",
    "avg_val_loss = np.mean([result['val_loss'] for result in fold_results])\n",
    "avg_mean_dice = np.mean([result['mean_dice'] for result in fold_results])\n",
    "avg_mean_precision = np.mean([result['mean_precision'] for result in fold_results])\n",
    "avg_mean_recall = np.mean([result['mean_recall'] for result in fold_results])\n",
    "\n",
    "# Output the final results\n",
    "print(\"Cross-validation results:\", fold_results)\n",
    "print(f\"Average Train Loss: {avg_train_loss}\")\n",
    "print(f\"Average Val Loss: {avg_val_loss}\")\n",
    "print(f\"Average Dice Score: {avg_mean_dice}\")\n",
    "print(f\"Average Precision: {avg_mean_precision}\")\n",
    "print(f\"Average Recall: {avg_mean_recall}\")\n",
    "\n",
    "# Output the best models\n",
    "print(f\"Best model based on Dice Score: {best_model_dice['path']} with score {best_model_dice['score']}\")\n",
    "print(f\"Best model based on Precision: {best_model_precision['path']} with score {best_model_precision['score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b958ce-d7ad-4a62-87bd-b0595400b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code for Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef555e3f-07cb-467f-9cee-85638d0660e1",
   "metadata": {},
   "source": [
    "### Old test code back up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fffd1d-2cca-4475-b3d6-5d43a441f28f",
   "metadata": {},
   "source": [
    "### Test new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5aec83f-4ca8-4220-99fb-9e69b416505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as coco_mask\n",
    "from transformers import SamModel, SamProcessor, SamConfig\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Determine the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "image_folder_path = \"Images/\"\n",
    "annotation_path = \"annotation/pcb1to15.json\"\n",
    "output_base_path = \"model_results/\"\n",
    "\n",
    "# Load the annotations\n",
    "with open(annotation_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Get all image file names\n",
    "image_files = [img['file_name'] for img in coco_data['images']]\n",
    "\n",
    "# Helper function to save split annotations\n",
    "def save_split_annotations(file_names, coco_data, output_path):\n",
    "    # Filter images\n",
    "    images = [img for img in coco_data['images'] if img['file_name'] in file_names]\n",
    "    image_ids = [img['id'] for img in images]\n",
    "\n",
    "    # Filter annotations\n",
    "    annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] in image_ids]\n",
    "\n",
    "    # Create new COCO data dictionary\n",
    "    new_coco_data = {\n",
    "        'info': coco_data['info'],\n",
    "        'licenses': coco_data['licenses'],\n",
    "        'images': images,\n",
    "        'annotations': annotations,\n",
    "        'categories': coco_data['categories']\n",
    "    }\n",
    "\n",
    "    # Save new annotations\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(new_coco_data, f, indent=4)\n",
    "\n",
    "# Helper function to copy images to their respective folders\n",
    "def copy_images(file_names, source_folder, destination_folder):\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    for file_name in file_names:\n",
    "        shutil.copy(os.path.join(source_folder, file_name), os.path.join(destination_folder, file_name))\n",
    "\n",
    "# Define data augmentation and normalization transforms\n",
    "class CustomTransforms:\n",
    "    def __init__(self, is_train=True):\n",
    "        if is_train:\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((1024, 1024)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((1024, 1024)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        image = self.transforms(image)\n",
    "        mask = Image.fromarray(mask)\n",
    "        mask = F.resize(mask, (1024, 1024), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        mask = transforms.ToTensor()(mask).long().squeeze(0)  # Ensure mask is 2D\n",
    "        return image, mask\n",
    "\n",
    "# Custom dataset class\n",
    "class PCBXRayDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        self.image_info = self.coco_data['images']\n",
    "        self.annotations = {ann['image_id']: ann for ann in self.coco_data['annotations']}\n",
    "        self.category_info = self.coco_data['categories']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.image_info[idx]\n",
    "        image_id = img_info['id']\n",
    "        image_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "        anns = [ann for ann in self.annotations.values() if ann['image_id'] == image_id]\n",
    "        for ann in anns:\n",
    "            rle = coco_mask.frPyObjects(ann['segmentation'], image.shape[0], image.shape[1])\n",
    "            decoded_mask = coco_mask.decode(rle)\n",
    "            if len(decoded_mask.shape) == 3:\n",
    "                decoded_mask = decoded_mask[:, :, 0]  # Take the first channel if it's a multi-channel mask\n",
    "            mask += np.squeeze(decoded_mask).astype(np.uint8)\n",
    "\n",
    "        if self.transforms:\n",
    "            image, mask = self.transforms(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "class SpatialAdapter(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SpatialAdapter, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "class SAMWithSpatialAdapters(nn.Module):\n",
    "    def __init__(self, sam_model, adapter_channels=256, num_classes=5):  # Add num_classes parameter\n",
    "        super(SAMWithSpatialAdapters, self).__init__()\n",
    "        self.sam_model = sam_model\n",
    "        # Initialize SpatialAdapter with 256 input and output channels\n",
    "        self.adapters = nn.ModuleList([SpatialAdapter(256, adapter_channels) for _ in range(sam_model.config.vision_config.num_hidden_layers)])\n",
    "        self.final_conv = nn.Conv2d(adapter_channels, num_classes, kernel_size=1)  # Change to num_classes\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        vision_outputs = self.sam_model.vision_encoder(pixel_values)\n",
    "        x = vision_outputs[0]  # Assuming vision_outputs[0] is the feature map\n",
    "        # print(f\"vision_outputs[0] shape: {x.shape}\")  # Debug print to inspect shape\n",
    "        for adapter in self.adapters:\n",
    "            x = adapter(x)\n",
    "        x = self.final_conv(x)  # Reduce to num_classes channels\n",
    "        # print(f\"Final output shape before upsampling: {x.shape}\")  # Debug print to inspect shape\n",
    "        return x\n",
    "\n",
    "# Load the pre-trained SAM model\n",
    "sam_model = SamModel.from_pretrained('facebook/sam-vit-huge')\n",
    "\n",
    "# Integrate spatial adapters\n",
    "model_with_adapters = SAMWithSpatialAdapters(sam_model).to(device)\n",
    "\n",
    "# Set up the optimizer with Layer-wise Learning Rate Decay\n",
    "def get_optimizer_with_llrd(model, base_lr=1e-4, lr_decay_factor=0.95):\n",
    "    seen_params = set()\n",
    "    optimizer_grouped_parameters = []\n",
    "    for i, layer in enumerate(model.sam_model.vision_encoder.layers):\n",
    "        layer_params = [p for n, p in layer.named_parameters() if p not in seen_params]\n",
    "        seen_params.update(layer_params)\n",
    "        optimizer_grouped_parameters.append({'params': layer_params, 'lr': base_lr * (lr_decay_factor ** (len(model.sam_model.vision_encoder.layers) - i))})\n",
    "\n",
    "    adapter_params = [p for n, p in model.named_parameters() if \"adapter\" in n and p not in seen_params]\n",
    "    seen_params.update(adapter_params)\n",
    "    optimizer_grouped_parameters.append({'params': adapter_params, 'lr': base_lr})\n",
    "\n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "    return optimizer\n",
    "\n",
    "optimizer = get_optimizer_with_llrd(model_with_adapters)\n",
    "\n",
    "# Dice Loss Function for Multiple Classes\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = torch.softmax(inputs, dim=1)  # Use softmax for multi-class segmentation\n",
    "        loss = 0\n",
    "        for c in range(self.num_classes):\n",
    "            input_c = inputs[:, c, :, :]\n",
    "            target_c = (targets == c).float()\n",
    "            intersection = (input_c * target_c).sum()\n",
    "            dice = (2. * intersection + smooth) / (input_c.sum() + target_c.sum() + smooth)\n",
    "            loss += 1 - dice\n",
    "        return loss / self.num_classes\n",
    "\n",
    "\n",
    "\n",
    "# Dice score function\n",
    "def dice_score(pred, target, smooth=1):\n",
    "    intersection = np.sum(pred * target)\n",
    "    return (2. * intersection + smooth) / (np.sum(pred) + np.sum(target) + smooth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e2dcc06-da30-4ef8-8fd1-4649b97098b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8187217069607154\n",
      "Test Dice Score: 1.0\n",
      "Test Precision: 0.2\n",
      "Test Recall: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model_on_test_set(model, data_loader, num_classes=5):\n",
    "    model.eval()\n",
    "    dice_scores = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    criterion = DiceLoss(num_classes=num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in data_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.nn.functional.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Ensure outputs and masks have the correct types\n",
    "            outputs = outputs.float()\n",
    "            masks = masks.long()\n",
    "            \n",
    "            outputs_softmax = torch.softmax(outputs, dim=1)  # Use softmax for multi-class segmentation\n",
    "\n",
    "            for c in range(num_classes):\n",
    "                pred_mask = (torch.argmax(outputs_softmax, dim=1) == c).float()\n",
    "                true_mask = (masks == c).float()\n",
    "                dice = dice_score(pred_mask.cpu().numpy(), true_mask.cpu().numpy())\n",
    "                dice_scores.append(dice)\n",
    "\n",
    "                precision = precision_score(true_mask.cpu().numpy().flatten(), pred_mask.cpu().numpy().flatten(), zero_division=0)\n",
    "                recall = recall_score(true_mask.cpu().numpy().flatten(), pred_mask.cpu().numpy().flatten(), zero_division=0)\n",
    "\n",
    "                precision_list.append(precision)\n",
    "                recall_list.append(recall)\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    mean_dice = np.mean(dice_scores)\n",
    "    mean_precision = np.mean(precision_list)\n",
    "    mean_recall = np.mean(recall_list)\n",
    "    test_loss = test_loss / len(data_loader)\n",
    "\n",
    "    return test_loss, mean_dice, mean_precision, mean_recall\n",
    "\n",
    "best_model_path = \"saved_best_model/best_model_epoch_25.pth\"\n",
    "test_image_folder_path = \"test/\"\n",
    "test_annotation_path = \"annotation/test211121516.json\"\n",
    "\n",
    "# Load the best model for evaluation on the test set\n",
    "best_model = SAMWithSpatialAdapters(sam_model, num_classes=5).to(device)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = PCBXRayDataset(test_image_folder_path, test_annotation_path, transforms=CustomTransforms(is_train=False))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Evaluate the best model on the test dataset\n",
    "test_loss, test_dice, test_precision, test_recall = evaluate_model_on_test_set(best_model, test_loader, num_classes=5)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Dice Score: {test_dice}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14661995-87c6-47c7-b1de-b708c1d8b309",
   "metadata": {},
   "source": [
    "### code for overlay class labels with the images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
